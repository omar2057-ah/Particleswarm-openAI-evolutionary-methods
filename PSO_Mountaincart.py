# -*- coding: utf-8 -*-
"""Copy of Copy of PSOMountainCart.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oiFVjUe14wDOb-jfhxzfmcJpOLmGtd4i
"""

#remove " > /dev/null 2>&1" to see what is going on under the hood
!pip install gym pyvirtualdisplay > /dev/null 2>&1
!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1
!apt-get update > /dev/null 2>&1
!apt-get install cmake > /dev/null 2>&1
!pip install --upgrade setuptools 2>&1
!pip install ez_setup > /dev/null 2>&1
!pip install gym[atari] > /dev/null 2>&1

# Commented out IPython magic to ensure Python compatibility.
from __future__ import absolute_import, division, print_function, unicode_literals
try:
  # %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
except Exception:
  pass
import tensorflow as tf
import gym
from gym import logger as gymlogger
from gym.wrappers import Monitor
gymlogger.set_level(40) #error only
import tensorflow as tf
import numpy as np
import random
import matplotlib
import matplotlib.pyplot as plt
# %matplotlib inline
import math
import glob
import io
import base64
from IPython.display import HTML

from IPython import display as ipythondisplay
from pyvirtualdisplay import Display
display = Display(visible=0, size=(1400, 900))
display.start()

def show_video():
  mp4list = glob.glob('video/*.mp4')
  if len(mp4list) > 0:
    mp4 = mp4list[0]
    video = io.open(mp4, 'r+b').read()
    encoded = base64.b64encode(video)
    ipythondisplay.display(HTML(data='''<video alt="test" autoplay 
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
             </video>'''.format(encoded.decode('ascii'))))
  else: 
    print("Could not find video")
          
def wrap_env(env):
  env = Monitor(env, './video', force=True)
  return env

env = wrap_env(gym.make("MountainCarContinuous-v0"))
print(env.action_space)
print(env.observation_space)
observation = env.reset()

while True:
  
    env.render()
    
    #your agent goes here
    action = env.action_space.sample() 
         
    observation, reward, done, info = env.step(action) 
   
        
    if done: 
      break;
            
env.close()
show_video()

# Commented out IPython magic to ensure Python compatibility.
!pip install pyswarms
import numpy as np
import matplotlib.pyplot as plt
import pyswarms as ps
# Some more magic so that the notebook will reload external python modules;
# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython
# %load_ext autoreload
# %autoreload 2

#Forward Propagation
def forward_prop(params,obs):
    # Neural network architecture
    n_inputs = env.observation_space.shape[0]
    
    n_hidden = 20 
    n_classes = env.action_space.shape[0]

    # Roll-back the weights and biases
    W1 = params[0:(n_inputs*20)].reshape((n_inputs,n_hidden))
    b1 = params[(n_inputs*20):(n_inputs*20)+20].reshape((n_hidden,))
    W2 = params[(n_inputs*20)+20:(n_inputs*20)+(n_hidden**2)+20].reshape((n_hidden,n_hidden))
    b2 = params[(n_inputs*20)+(n_hidden**2)+20:(n_inputs*20)+(n_hidden**2)+(20*2)].reshape((n_hidden,))
    W3 = params[(n_inputs*20)+(n_hidden**2)+(20*2):(n_inputs*20)+(2*(n_hidden**2))+(20*2)].reshape((n_hidden,n_hidden))
    b3 = params[(n_inputs*20)+(2*(n_hidden**2))+(20*2):(n_inputs*20)+(2*(n_hidden**2))+(20*3)].reshape((n_hidden,))
    W4 = params[(n_inputs*20)+(2*(n_hidden**2))+(20*3):(n_inputs*20)+(2*(n_hidden**2))+(20*4)].reshape((n_hidden,n_classes))
    b4 = params[(n_inputs*20)+(2*(n_hidden**2))+(20*4):(n_inputs*20)+(2*(n_hidden**2))+(20*4)+1].reshape((n_classes,))


    # Perform forward propagation
    z1 = obs.dot(W1) + b1  # Pre-activation in Layer 1
    a1 = np.tanh(z1)       # Activation in Layer 1
    z2 = a1.dot(W2) + b2   # Pre-activation in Layer 2
    a2 = np.tanh(z2) 
    z3 = a2.dot(W3) + b3
    a3 = np.tanh(z3)
    z4 = a3.dot(W4) + b4
    opaction = np.tanh(z4)
    #OutputAction = np.argmax(opaction)

    return opaction

#F(X)
def f(x):
    MAX_STEPS = 500
    totalReward=0
    j = np.array([])
    n_particles = x.shape[0]
    for i in range(n_particles):
        observation = env.reset()
        for step in range(MAX_STEPS):
              #env.render()
              action = forward_prop(x[i],observation)
              observation, reward, done, info = env.step(action)
              print(reward)
              totalReward += reward  

              if done:
                  break
        TR = -totalReward
        j=np.append(j,TR)
    return j

# Commented out IPython magic to ensure Python compatibility.
# #Main
# %%time
# # Initialize swarm
# options = {'c1': 0.5, 'c2': 0.3, 'w':0.9}
# in_dimen = env.observation_space.shape[0] 
# out_dimen = env.action_space.shape[0]
# # Call instance of PSO0
# dimensions = (in_dimen * 20) + 20 + (20*20) +20 +(20*20) + 20 + (20 * out_dimen) + out_dimen
# optimizer = ps.single.GlobalBestPSO(n_particles=100, dimensions=dimensions, options=options)
# 
# # Perform optimization
# cost, pos = optimizer.optimize(f, iters=500)

from pyswarms.utils.plotters import plot_cost_history, plot_contour, plot_surface
import matplotlib.pyplot as plt
plot_cost_history(optimizer.cost_history)
plt.show()

0observation = env.reset()
totalReward = 0
for step in range(1000):
              env.render()
              action = forward_prop(pos,observation)
              observation, reward, done, info = env.step(action)
              totalReward += reward  
              print(totalReward)
              if done:
                observation = env.reset()
                break
env.close()
show_video()